layer {
  name: "input"
  type: "Input"
  top: "input"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 224
      dim: 416
    }
  }
}
layer {
  name: "Conv_0"
  type: "Convolution"
  bottom: "input"
  top: "778"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "Relu_1"
  type: "ReLU"
  bottom: "778"
  top: "503"
}
layer {
  name: "Conv_2"
  type: "Convolution"
  bottom: "503"
  top: "781"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_3"
  type: "ReLU"
  bottom: "781"
  top: "506"
}
layer {
  name: "Conv_4"
  type: "Convolution"
  bottom: "506"
  top: "784"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 64
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_5"
  type: "ReLU"
  bottom: "784"
  top: "509"
}
layer {
  name: "Conv_6"
  type: "Convolution"
  bottom: "509"
  top: "787"
  convolution_param {
    num_output: 16
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_7"
  type: "Convolution"
  bottom: "503"
  top: "790"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 32
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_8"
  type: "ReLU"
  bottom: "790"
  top: "514"
}
layer {
  name: "Concat_9"
  type: "Concat"
  bottom: "787"
  bottom: "514"
  top: "515"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_10"
  type: "Convolution"
  bottom: "515"
  top: "793"
  convolution_param {
    num_output: 48
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_11"
  type: "ReLU"
  bottom: "793"
  top: "518"
}
layer {
  name: "Conv_12"
  type: "Convolution"
  bottom: "518"
  top: "796"
  convolution_param {
    num_output: 48
    bias_term: true
    group: 48
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "Relu_13"
  type: "ReLU"
  bottom: "796"
  top: "521"
}
layer {
  name: "Conv_14"
  type: "Convolution"
  bottom: "521"
  top: "799"
  convolution_param {
    num_output: 24
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_15"
  type: "Convolution"
  bottom: "799"
  top: "802"
  convolution_param {
    num_output: 96
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_16"
  type: "ReLU"
  bottom: "802"
  top: "526"
}
layer {
  name: "Conv_17"
  type: "Convolution"
  bottom: "526"
  top: "805"
  convolution_param {
    num_output: 96
    bias_term: true
    group: 96
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_18"
  type: "ReLU"
  bottom: "805"
  top: "529"
}
layer {
  name: "Conv_19"
  type: "Convolution"
  bottom: "529"
  top: "808"
  convolution_param {
    num_output: 24
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Add_20"
  type: "Eltwise"
  bottom: "799"
  bottom: "808"
  top: "532"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_21"
  type: "Convolution"
  bottom: "518"
  top: "811"
  convolution_param {
    num_output: 48
    bias_term: true
    group: 48
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "Relu_22"
  type: "ReLU"
  bottom: "811"
  top: "535"
}
layer {
  name: "Concat_23"
  type: "Concat"
  bottom: "532"
  bottom: "535"
  top: "536"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_24"
  type: "Convolution"
  bottom: "536"
  top: "814"
  convolution_param {
    num_output: 72
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_25"
  type: "ReLU"
  bottom: "814"
  top: "539"
}
layer {
  name: "Conv_26"
  type: "Convolution"
  bottom: "539"
  top: "817"
  convolution_param {
    num_output: 72
    bias_term: true
    group: 72
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "Relu_27"
  type: "ReLU"
  bottom: "817"
  top: "542"
}
layer {
  name: "Conv_28"
  type: "Convolution"
  bottom: "542"
  top: "820"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_29"
  type: "Convolution"
  bottom: "820"
  top: "823"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_30"
  type: "ReLU"
  bottom: "823"
  top: "547"
}
layer {
  name: "Conv_31"
  type: "Convolution"
  bottom: "547"
  top: "826"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 128
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_32"
  type: "ReLU"
  bottom: "826"
  top: "550"
}
layer {
  name: "Conv_33"
  type: "Convolution"
  bottom: "550"
  top: "829"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Add_34"
  type: "Eltwise"
  bottom: "820"
  bottom: "829"
  top: "553"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_35"
  type: "Convolution"
  bottom: "553"
  top: "832"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_36"
  type: "ReLU"
  bottom: "832"
  top: "556"
}
layer {
  name: "Conv_37"
  type: "Convolution"
  bottom: "556"
  top: "835"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 128
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_38"
  type: "ReLU"
  bottom: "835"
  top: "559"
}
layer {
  name: "Conv_39"
  type: "Convolution"
  bottom: "559"
  top: "838"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Add_40"
  type: "Eltwise"
  bottom: "553"
  bottom: "838"
  top: "562"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_41"
  type: "Convolution"
  bottom: "562"
  top: "841"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_42"
  type: "ReLU"
  bottom: "841"
  top: "565"
}
layer {
  name: "Conv_43"
  type: "Convolution"
  bottom: "565"
  top: "844"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 128
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_44"
  type: "ReLU"
  bottom: "844"
  top: "568"
}
layer {
  name: "Conv_45"
  type: "Convolution"
  bottom: "568"
  top: "847"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_46"
  type: "Convolution"
  bottom: "847"
  top: "850"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_47"
  type: "ReLU"
  bottom: "850"
  top: "573"
}
layer {
  name: "Conv_48"
  type: "Convolution"
  bottom: "573"
  top: "853"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 256
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_49"
  type: "ReLU"
  bottom: "853"
  top: "576"
}
layer {
  name: "Conv_50"
  type: "Convolution"
  bottom: "576"
  top: "856"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Add_51"
  type: "Eltwise"
  bottom: "847"
  bottom: "856"
  top: "579"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_52"
  type: "Convolution"
  bottom: "579"
  top: "859"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_53"
  type: "ReLU"
  bottom: "859"
  top: "582"
}
layer {
  name: "Conv_54"
  type: "Convolution"
  bottom: "582"
  top: "862"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 256
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_55"
  type: "ReLU"
  bottom: "862"
  top: "585"
}
layer {
  name: "Conv_56"
  type: "Convolution"
  bottom: "585"
  top: "865"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Add_57"
  type: "Eltwise"
  bottom: "579"
  bottom: "865"
  top: "588"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_58"
  type: "Convolution"
  bottom: "588"
  top: "868"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_59"
  type: "ReLU"
  bottom: "868"
  top: "591"
}
layer {
  name: "Conv_60"
  type: "Convolution"
  bottom: "591"
  top: "871"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 256
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_61"
  type: "ReLU"
  bottom: "871"
  top: "594"
}
layer {
  name: "Conv_62"
  type: "Convolution"
  bottom: "594"
  top: "874"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Add_63"
  type: "Eltwise"
  bottom: "588"
  bottom: "874"
  top: "597"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_64"
  type: "Convolution"
  bottom: "539"
  top: "877"
  convolution_param {
    num_output: 72
    bias_term: true
    group: 72
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "Relu_65"
  type: "ReLU"
  bottom: "877"
  top: "600"
}
layer {
  name: "Concat_66"
  type: "Concat"
  bottom: "597"
  bottom: "600"
  top: "601"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_67"
  type: "Convolution"
  bottom: "601"
  top: "880"
  convolution_param {
    num_output: 192
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_68"
  type: "ReLU"
  bottom: "880"
  top: "604"
}
layer {
  name: "Conv_69"
  type: "Convolution"
  bottom: "604"
  top: "883"
  convolution_param {
    num_output: 192
    bias_term: true
    group: 192
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "Relu_70"
  type: "ReLU"
  bottom: "883"
  top: "607"
}
layer {
  name: "Conv_71"
  type: "Convolution"
  bottom: "607"
  top: "886"
  convolution_param {
    num_output: 96
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_72"
  type: "Convolution"
  bottom: "886"
  top: "889"
  convolution_param {
    num_output: 384
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_73"
  type: "ReLU"
  bottom: "889"
  top: "612"
}
layer {
  name: "Conv_74"
  type: "Convolution"
  bottom: "612"
  top: "892"
  convolution_param {
    num_output: 384
    bias_term: true
    group: 384
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_75"
  type: "ReLU"
  bottom: "892"
  top: "615"
}
layer {
  name: "Conv_76"
  type: "Convolution"
  bottom: "615"
  top: "895"
  convolution_param {
    num_output: 96
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Add_77"
  type: "Eltwise"
  bottom: "886"
  bottom: "895"
  top: "618"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_78"
  type: "Convolution"
  bottom: "618"
  top: "898"
  convolution_param {
    num_output: 384
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_79"
  type: "ReLU"
  bottom: "898"
  top: "621"
}
layer {
  name: "Conv_80"
  type: "Convolution"
  bottom: "621"
  top: "901"
  convolution_param {
    num_output: 384
    bias_term: true
    group: 384
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_81"
  type: "ReLU"
  bottom: "901"
  top: "624"
}
layer {
  name: "Conv_82"
  type: "Convolution"
  bottom: "624"
  top: "904"
  convolution_param {
    num_output: 96
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Add_83"
  type: "Eltwise"
  bottom: "618"
  bottom: "904"
  top: "627"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_84"
  type: "Convolution"
  bottom: "604"
  top: "907"
  convolution_param {
    num_output: 192
    bias_term: true
    group: 192
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "Relu_85"
  type: "ReLU"
  bottom: "907"
  top: "630"
}
layer {
  name: "Concat_86"
  type: "Concat"
  bottom: "627"
  bottom: "630"
  top: "631"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_87"
  type: "Convolution"
  bottom: "631"
  top: "910"
  convolution_param {
    num_output: 288
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_88"
  type: "ReLU"
  bottom: "910"
  top: "634"
}
layer {
  name: "Conv_89"
  type: "Convolution"
  bottom: "634"
  top: "913"
  convolution_param {
    num_output: 288
    bias_term: true
    group: 288
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "Relu_90"
  type: "ReLU"
  bottom: "913"
  top: "637"
}
layer {
  name: "Conv_91"
  type: "Convolution"
  bottom: "637"
  top: "916"
  convolution_param {
    num_output: 160
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_92"
  type: "Convolution"
  bottom: "916"
  top: "919"
  convolution_param {
    num_output: 640
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_93"
  type: "ReLU"
  bottom: "919"
  top: "642"
}
layer {
  name: "Conv_94"
  type: "Convolution"
  bottom: "642"
  top: "922"
  convolution_param {
    num_output: 640
    bias_term: true
    group: 640
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_95"
  type: "ReLU"
  bottom: "922"
  top: "645"
}
layer {
  name: "Conv_96"
  type: "Convolution"
  bottom: "645"
  top: "925"
  convolution_param {
    num_output: 160
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Add_97"
  type: "Eltwise"
  bottom: "916"
  bottom: "925"
  top: "648"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_98"
  type: "Convolution"
  bottom: "648"
  top: "928"
  convolution_param {
    num_output: 640
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_99"
  type: "ReLU"
  bottom: "928"
  top: "651"
}
layer {
  name: "Conv_100"
  type: "Convolution"
  bottom: "651"
  top: "931"
  convolution_param {
    num_output: 640
    bias_term: true
    group: 640
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_101"
  type: "ReLU"
  bottom: "931"
  top: "654"
}
layer {
  name: "Conv_102"
  type: "Convolution"
  bottom: "654"
  top: "934"
  convolution_param {
    num_output: 160
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Add_103"
  type: "Eltwise"
  bottom: "648"
  bottom: "934"
  top: "657"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_104"
  type: "Convolution"
  bottom: "657"
  top: "937"
  convolution_param {
    num_output: 640
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_105"
  type: "ReLU"
  bottom: "937"
  top: "660"
}
layer {
  name: "Conv_106"
  type: "Convolution"
  bottom: "660"
  top: "940"
  convolution_param {
    num_output: 640
    bias_term: true
    group: 640
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_107"
  type: "ReLU"
  bottom: "940"
  top: "663"
}
layer {
  name: "Conv_108"
  type: "Convolution"
  bottom: "663"
  top: "943"
  convolution_param {
    num_output: 320
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_109"
  type: "Convolution"
  bottom: "634"
  top: "946"
  convolution_param {
    num_output: 288
    bias_term: true
    group: 288
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "Relu_110"
  type: "ReLU"
  bottom: "946"
  top: "668"
}
layer {
  name: "Concat_111"
  type: "Concat"
  bottom: "943"
  bottom: "668"
  top: "669"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_112"
  type: "Convolution"
  bottom: "669"
  top: "949"
  convolution_param {
    num_output: 640
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_113"
  type: "ReLU"
  bottom: "949"
  top: "672"
}
layer {
  name: "Conv_114"
  type: "Convolution"
  bottom: "604"
  top: "952"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_115"
  type: "ReLU"
  bottom: "952"
  top: "675"
}
layer {
  name: "Conv_116"
  type: "Convolution"
  bottom: "634"
  top: "955"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_117"
  type: "ReLU"
  bottom: "955"
  top: "678"
}
layer {
  name: "Conv_118"
  type: "Convolution"
  bottom: "672"
  top: "958"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_119"
  type: "ReLU"
  bottom: "958"
  top: "681"
}
layer {
  name: "Resize_121"
  type: "Deconvolution"
  bottom: "681"
  top: "686"
  convolution_param {
    num_output: 128
    bias_term: false
    group: 128
    weight_filler {
      type: "bilinear"
    }
    pad_h: 1
    pad_w: 1
    kernel_h: 4
    kernel_w: 4
    stride_h: 2
    stride_w: 2
  }
}
layer {
  name: "Add_122"
  type: "Eltwise"
  bottom: "678"
  bottom: "686"
  top: "687"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Resize_124"
  type: "Deconvolution"
  bottom: "687"
  top: "692"
  convolution_param {
    num_output: 128
    bias_term: false
    group: 128
    weight_filler {
      type: "bilinear"
    }
    pad_h: 1
    pad_w: 1
    kernel_h: 4
    kernel_w: 4
    stride_h: 2
    stride_w: 2
  }
}
layer {
  name: "Add_125"
  type: "Eltwise"
  bottom: "675"
  bottom: "692"
  top: "693"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Resize_127"
  type: "Pooling"
  bottom: "693"
  top: "698"
  pooling_param {
    pool: AVE
    kernel_h: 2
    kernel_w: 2
    stride_h: 2
    stride_w: 2
    pad_h: 0
    pad_w: 0
  }
}
layer {
  name: "Add_128"
  type: "Eltwise"
  bottom: "687"
  bottom: "698"
  top: "699"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Resize_130"
  type: "Pooling"
  bottom: "699"
  top: "704"
  pooling_param {
    pool: AVE
    kernel_h: 2
    kernel_w: 2
    stride_h: 2
    stride_w: 2
    pad_h: 0
    pad_w: 0
  }
}
layer {
  name: "Add_131"
  type: "Eltwise"
  bottom: "681"
  bottom: "704"
  top: "705"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_132"
  type: "Convolution"
  bottom: "693"
  top: "961"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_133"
  type: "ReLU"
  bottom: "961"
  top: "708"
}
layer {
  name: "Conv_134"
  type: "Convolution"
  bottom: "708"
  top: "964"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 128
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_135"
  type: "ReLU"
  bottom: "964"
  top: "711"
}
layer {
  name: "Conv_136"
  type: "Convolution"
  bottom: "711"
  top: "967"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_137"
  type: "ReLU"
  bottom: "967"
  top: "714"
}
layer {
  name: "Conv_138"
  type: "Convolution"
  bottom: "714"
  top: "970"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 128
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_139"
  type: "ReLU"
  bottom: "970"
  top: "717"
}
layer {
  name: "Conv_140"
  type: "Convolution"
  bottom: "717"
  top: "718"
  convolution_param {
    num_output: 3
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_141"
  type: "Convolution"
  bottom: "708"
  top: "973"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 128
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_142"
  type: "ReLU"
  bottom: "973"
  top: "721"
}
layer {
  name: "Conv_143"
  type: "Convolution"
  bottom: "721"
  top: "976"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_144"
  type: "ReLU"
  bottom: "976"
  top: "724"
}
layer {
  name: "Conv_145"
  type: "Convolution"
  bottom: "724"
  top: "979"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 128
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_146"
  type: "ReLU"
  bottom: "979"
  top: "727"
}
layer {
  name: "Conv_147"
  type: "Convolution"
  bottom: "727"
  top: "728"
  convolution_param {
    num_output: 5
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Concat_148"
  type: "Concat"
  bottom: "728"
  bottom: "718"
  top: "output.1"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_149"
  type: "Convolution"
  bottom: "699"
  top: "982"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_150"
  type: "ReLU"
  bottom: "982"
  top: "732"
}
layer {
  name: "Conv_151"
  type: "Convolution"
  bottom: "732"
  top: "985"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 128
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_152"
  type: "ReLU"
  bottom: "985"
  top: "735"
}
layer {
  name: "Conv_153"
  type: "Convolution"
  bottom: "735"
  top: "988"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_154"
  type: "ReLU"
  bottom: "988"
  top: "738"
}
layer {
  name: "Conv_155"
  type: "Convolution"
  bottom: "738"
  top: "991"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 128
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_156"
  type: "ReLU"
  bottom: "991"
  top: "741"
}
layer {
  name: "Conv_157"
  type: "Convolution"
  bottom: "741"
  top: "742"
  convolution_param {
    num_output: 3
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_158"
  type: "Convolution"
  bottom: "732"
  top: "994"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 128
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_159"
  type: "ReLU"
  bottom: "994"
  top: "745"
}
layer {
  name: "Conv_160"
  type: "Convolution"
  bottom: "745"
  top: "997"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_161"
  type: "ReLU"
  bottom: "997"
  top: "748"
}
layer {
  name: "Conv_162"
  type: "Convolution"
  bottom: "748"
  top: "1000"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 128
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_163"
  type: "ReLU"
  bottom: "1000"
  top: "751"
}
layer {
  name: "Conv_164"
  type: "Convolution"
  bottom: "751"
  top: "752"
  convolution_param {
    num_output: 5
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Concat_165"
  type: "Concat"
  bottom: "752"
  bottom: "742"
  top: "output.2"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_166"
  type: "Convolution"
  bottom: "705"
  top: "1003"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_167"
  type: "ReLU"
  bottom: "1003"
  top: "756"
}
layer {
  name: "Conv_168"
  type: "Convolution"
  bottom: "756"
  top: "1006"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 128
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_169"
  type: "ReLU"
  bottom: "1006"
  top: "759"
}
layer {
  name: "Conv_170"
  type: "Convolution"
  bottom: "759"
  top: "1009"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_171"
  type: "ReLU"
  bottom: "1009"
  top: "762"
}
layer {
  name: "Conv_172"
  type: "Convolution"
  bottom: "762"
  top: "1012"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 128
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_173"
  type: "ReLU"
  bottom: "1012"
  top: "765"
}
layer {
  name: "Conv_174"
  type: "Convolution"
  bottom: "765"
  top: "766"
  convolution_param {
    num_output: 3
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_175"
  type: "Convolution"
  bottom: "756"
  top: "1015"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 128
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_176"
  type: "ReLU"
  bottom: "1015"
  top: "769"
}
layer {
  name: "Conv_177"
  type: "Convolution"
  bottom: "769"
  top: "1018"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_178"
  type: "ReLU"
  bottom: "1018"
  top: "772"
}
layer {
  name: "Conv_179"
  type: "Convolution"
  bottom: "772"
  top: "1021"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 128
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Relu_180"
  type: "ReLU"
  bottom: "1021"
  top: "775"
}
layer {
  name: "Conv_181"
  type: "Convolution"
  bottom: "775"
  top: "776"
  convolution_param {
    num_output: 5
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Concat_182"
  type: "Concat"
  bottom: "776"
  bottom: "766"
  top: "output.3"
  concat_param {
    axis: 1
  }
}
