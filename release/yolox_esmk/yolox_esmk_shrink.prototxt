layer {
  name: "input"
  type: "Input"
  top: "input"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 224
      dim: 416
    }
  }
}
layer {
  name: "Conv_0"
  type: "Convolution"
  bottom: "input"
  top: "922"
  convolution_param {
    num_output: 24
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_1"
  type: "ReLU"
  bottom: "922"
  top: "605"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "MaxPool_2"
  type: "Pooling"
  bottom: "605"
  top: "606"
  pooling_param {
    pool: MAX
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    pad_h: 0
    pad_w: 0
  }
}
layer {
  name: "Conv_3"
  type: "Convolution"
  bottom: "606"
  top: "925"
  convolution_param {
    num_output: 28
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_4"
  type: "ReLU"
  bottom: "925"
  top: "609"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_5"
  type: "Convolution"
  bottom: "609"
  top: "928"
  convolution_param {
    num_output: 28
    bias_term: true
    group: 28
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "Conv_6"
  type: "Convolution"
  bottom: "928"
  top: "931"
  convolution_param {
    num_output: 16
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_7"
  type: "ReLU"
  bottom: "931"
  top: "614"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_8"
  type: "Convolution"
  bottom: "606"
  top: "934"
  convolution_param {
    num_output: 24
    bias_term: true
    group: 24
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "Conv_9"
  type: "Convolution"
  bottom: "934"
  top: "937"
  convolution_param {
    num_output: 16
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_10"
  type: "ReLU"
  bottom: "937"
  top: "619"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Concat_11"
  type: "Concat"
  bottom: "614"
  bottom: "619"
  top: "620"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_12"
  type: "Convolution"
  bottom: "620"
  top: "940"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 32
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_13"
  type: "ReLU"
  bottom: "940"
  top: "623"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_14"
  type: "Convolution"
  bottom: "623"
  top: "943"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_15"
  type: "ReLU"
  bottom: "943"
  top: "626"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_16"
  type: "Convolution"
  bottom: "626"
  top: "946"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_17"
  type: "ReLU"
  bottom: "946"
  top: "629"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_18"
  type: "Convolution"
  bottom: "629"
  top: "949"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 128
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_19"
  type: "ReLU"
  bottom: "949"
  top: "632"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_20"
  type: "Convolution"
  bottom: "632"
  top: "952"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Add_21"
  type: "Eltwise"
  bottom: "952"
  bottom: "626"
  top: "635"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_22"
  type: "Convolution"
  bottom: "635"
  top: "955"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_23"
  type: "ReLU"
  bottom: "955"
  top: "638"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_24"
  type: "Convolution"
  bottom: "638"
  top: "958"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 128
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_25"
  type: "ReLU"
  bottom: "958"
  top: "641"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_26"
  type: "Convolution"
  bottom: "641"
  top: "961"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Add_27"
  type: "Eltwise"
  bottom: "961"
  bottom: "635"
  top: "644"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_28"
  type: "Convolution"
  bottom: "644"
  top: "964"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_29"
  type: "ReLU"
  bottom: "964"
  top: "647"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_30"
  type: "Convolution"
  bottom: "647"
  top: "967"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 32
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "Conv_31"
  type: "Convolution"
  bottom: "967"
  top: "970"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_32"
  type: "ReLU"
  bottom: "970"
  top: "652"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_33"
  type: "Convolution"
  bottom: "644"
  top: "973"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 32
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "Conv_34"
  type: "Convolution"
  bottom: "973"
  top: "976"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_35"
  type: "ReLU"
  bottom: "976"
  top: "657"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Concat_36"
  type: "Concat"
  bottom: "652"
  bottom: "657"
  top: "658"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_37"
  type: "Convolution"
  bottom: "658"
  top: "979"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 64
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_38"
  type: "ReLU"
  bottom: "979"
  top: "661"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_39"
  type: "Convolution"
  bottom: "661"
  top: "982"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_40"
  type: "ReLU"
  bottom: "982"
  top: "664"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_41"
  type: "Convolution"
  bottom: "664"
  top: "985"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_42"
  type: "ReLU"
  bottom: "985"
  top: "667"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_43"
  type: "Convolution"
  bottom: "667"
  top: "988"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 256
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_44"
  type: "ReLU"
  bottom: "988"
  top: "670"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_45"
  type: "Convolution"
  bottom: "670"
  top: "991"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Add_46"
  type: "Eltwise"
  bottom: "991"
  bottom: "664"
  top: "673"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_47"
  type: "Convolution"
  bottom: "673"
  top: "994"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_48"
  type: "ReLU"
  bottom: "994"
  top: "676"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_49"
  type: "Convolution"
  bottom: "676"
  top: "997"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 256
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_50"
  type: "ReLU"
  bottom: "997"
  top: "679"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_51"
  type: "Convolution"
  bottom: "679"
  top: "1000"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Add_52"
  type: "Eltwise"
  bottom: "1000"
  bottom: "673"
  top: "682"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_53"
  type: "Convolution"
  bottom: "682"
  top: "1003"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_54"
  type: "ReLU"
  bottom: "1003"
  top: "685"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_55"
  type: "Convolution"
  bottom: "685"
  top: "1006"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 256
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_56"
  type: "ReLU"
  bottom: "1006"
  top: "688"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_57"
  type: "Convolution"
  bottom: "688"
  top: "1009"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Add_58"
  type: "Eltwise"
  bottom: "1009"
  bottom: "682"
  top: "691"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_59"
  type: "Convolution"
  bottom: "691"
  top: "1012"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_60"
  type: "ReLU"
  bottom: "1012"
  top: "694"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_61"
  type: "Convolution"
  bottom: "694"
  top: "1015"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 256
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_62"
  type: "ReLU"
  bottom: "1015"
  top: "697"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_63"
  type: "Convolution"
  bottom: "697"
  top: "1018"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Add_64"
  type: "Eltwise"
  bottom: "1018"
  bottom: "691"
  top: "700"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_65"
  type: "Convolution"
  bottom: "700"
  top: "1021"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_66"
  type: "ReLU"
  bottom: "1021"
  top: "703"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_67"
  type: "Convolution"
  bottom: "703"
  top: "1024"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 256
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_68"
  type: "ReLU"
  bottom: "1024"
  top: "706"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_69"
  type: "Convolution"
  bottom: "706"
  top: "1027"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Add_70"
  type: "Eltwise"
  bottom: "1027"
  bottom: "700"
  top: "709"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_71"
  type: "Convolution"
  bottom: "709"
  top: "1030"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_72"
  type: "ReLU"
  bottom: "1030"
  top: "712"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_73"
  type: "Convolution"
  bottom: "712"
  top: "1033"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 256
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_74"
  type: "ReLU"
  bottom: "1033"
  top: "715"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_75"
  type: "Convolution"
  bottom: "715"
  top: "1036"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Add_76"
  type: "Eltwise"
  bottom: "1036"
  bottom: "709"
  top: "718"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_77"
  type: "Convolution"
  bottom: "718"
  top: "1039"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_78"
  type: "ReLU"
  bottom: "1039"
  top: "721"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_79"
  type: "Convolution"
  bottom: "721"
  top: "1042"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 64
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "Conv_80"
  type: "Convolution"
  bottom: "1042"
  top: "1045"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_81"
  type: "ReLU"
  bottom: "1045"
  top: "726"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_82"
  type: "Convolution"
  bottom: "718"
  top: "1048"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 64
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "Conv_83"
  type: "Convolution"
  bottom: "1048"
  top: "1051"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_84"
  type: "ReLU"
  bottom: "1051"
  top: "731"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Concat_85"
  type: "Concat"
  bottom: "726"
  bottom: "731"
  top: "732"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_86"
  type: "Convolution"
  bottom: "732"
  top: "1054"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 128
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_87"
  type: "ReLU"
  bottom: "1054"
  top: "735"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_88"
  type: "Convolution"
  bottom: "735"
  top: "1057"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_89"
  type: "ReLU"
  bottom: "1057"
  top: "738"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_90"
  type: "Convolution"
  bottom: "738"
  top: "1060"
  convolution_param {
    num_output: 512
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_91"
  type: "ReLU"
  bottom: "1060"
  top: "741"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_92"
  type: "Convolution"
  bottom: "741"
  top: "1063"
  convolution_param {
    num_output: 512
    bias_term: true
    group: 512
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_93"
  type: "ReLU"
  bottom: "1063"
  top: "744"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_94"
  type: "Convolution"
  bottom: "744"
  top: "1066"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Add_95"
  type: "Eltwise"
  bottom: "1066"
  bottom: "738"
  top: "747"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_96"
  type: "Convolution"
  bottom: "747"
  top: "1069"
  convolution_param {
    num_output: 512
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_97"
  type: "ReLU"
  bottom: "1069"
  top: "750"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_98"
  type: "Convolution"
  bottom: "750"
  top: "1072"
  convolution_param {
    num_output: 512
    bias_term: true
    group: 512
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_99"
  type: "ReLU"
  bottom: "1072"
  top: "753"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_100"
  type: "Convolution"
  bottom: "753"
  top: "1075"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Add_101"
  type: "Eltwise"
  bottom: "1075"
  bottom: "747"
  top: "756"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_102"
  type: "Convolution"
  bottom: "644"
  top: "1078"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_103"
  type: "ReLU"
  bottom: "1078"
  top: "759"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_104"
  type: "Convolution"
  bottom: "718"
  top: "1081"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_105"
  type: "ReLU"
  bottom: "1081"
  top: "762"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_106"
  type: "Convolution"
  bottom: "756"
  top: "1084"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_107"
  type: "ReLU"
  bottom: "1084"
  top: "765"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Resize_109"
  type: "Deconvolution"
  bottom: "765"
  top: "770"
  convolution_param {
    num_output: 64
    bias_term: false
    group: 64
    weight_filler {
      type: "bilinear"
    }
    pad_h: 1
    pad_w: 1
    kernel_h: 4
    kernel_w: 4
    stride_h: 2
    stride_w: 2
  }
}
layer {
  name: "Concat_110"
  type: "Concat"
  bottom: "762"
  bottom: "770"
  top: "771"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_111"
  type: "Convolution"
  bottom: "771"
  top: "1087"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_112"
  type: "ReLU"
  bottom: "1087"
  top: "774"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_113"
  type: "Convolution"
  bottom: "771"
  top: "1090"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_114"
  type: "ReLU"
  bottom: "1090"
  top: "777"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_115"
  type: "Convolution"
  bottom: "777"
  top: "1093"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_116"
  type: "ReLU"
  bottom: "1093"
  top: "780"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_117"
  type: "Convolution"
  bottom: "780"
  top: "1096"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 128
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_118"
  type: "ReLU"
  bottom: "1096"
  top: "783"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_119"
  type: "Convolution"
  bottom: "783"
  top: "1099"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Add_120"
  type: "Eltwise"
  bottom: "1099"
  bottom: "777"
  top: "786"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Concat_121"
  type: "Concat"
  bottom: "774"
  bottom: "786"
  top: "787"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_122"
  type: "Convolution"
  bottom: "787"
  top: "1102"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_123"
  type: "ReLU"
  bottom: "1102"
  top: "790"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Resize_125"
  type: "Deconvolution"
  bottom: "790"
  top: "795"
  convolution_param {
    num_output: 64
    bias_term: false
    group: 64
    weight_filler {
      type: "bilinear"
    }
    pad_h: 1
    pad_w: 1
    kernel_h: 4
    kernel_w: 4
    stride_h: 2
    stride_w: 2
  }
}
layer {
  name: "Concat_126"
  type: "Concat"
  bottom: "759"
  bottom: "795"
  top: "796"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_127"
  type: "Convolution"
  bottom: "796"
  top: "1105"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_128"
  type: "ReLU"
  bottom: "1105"
  top: "799"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_129"
  type: "Convolution"
  bottom: "796"
  top: "1108"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_130"
  type: "ReLU"
  bottom: "1108"
  top: "802"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_131"
  type: "Convolution"
  bottom: "802"
  top: "1111"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_132"
  type: "ReLU"
  bottom: "1111"
  top: "805"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_133"
  type: "Convolution"
  bottom: "805"
  top: "1114"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 128
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_134"
  type: "ReLU"
  bottom: "1114"
  top: "808"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_135"
  type: "Convolution"
  bottom: "808"
  top: "1117"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Add_136"
  type: "Eltwise"
  bottom: "1117"
  bottom: "802"
  top: "811"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Concat_137"
  type: "Concat"
  bottom: "799"
  bottom: "811"
  top: "812"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_138"
  type: "Convolution"
  bottom: "812"
  top: "1120"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_139"
  type: "ReLU"
  bottom: "1120"
  top: "815"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_140"
  type: "Convolution"
  bottom: "815"
  top: "1123"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 64
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_141"
  type: "ReLU"
  bottom: "1123"
  top: "818"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_142"
  type: "Convolution"
  bottom: "818"
  top: "1126"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_143"
  type: "ReLU"
  bottom: "1126"
  top: "821"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Concat_144"
  type: "Concat"
  bottom: "790"
  bottom: "821"
  top: "822"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_145"
  type: "Convolution"
  bottom: "822"
  top: "1129"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_146"
  type: "ReLU"
  bottom: "1129"
  top: "825"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_147"
  type: "Convolution"
  bottom: "822"
  top: "1132"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_148"
  type: "ReLU"
  bottom: "1132"
  top: "828"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_149"
  type: "Convolution"
  bottom: "828"
  top: "1135"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_150"
  type: "ReLU"
  bottom: "1135"
  top: "831"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_151"
  type: "Convolution"
  bottom: "831"
  top: "1138"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 128
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_152"
  type: "ReLU"
  bottom: "1138"
  top: "834"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_153"
  type: "Convolution"
  bottom: "834"
  top: "1141"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Add_154"
  type: "Eltwise"
  bottom: "1141"
  bottom: "828"
  top: "837"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Concat_155"
  type: "Concat"
  bottom: "825"
  bottom: "837"
  top: "838"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_156"
  type: "Convolution"
  bottom: "838"
  top: "1144"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_157"
  type: "ReLU"
  bottom: "1144"
  top: "841"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_158"
  type: "Convolution"
  bottom: "841"
  top: "1147"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 64
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_159"
  type: "ReLU"
  bottom: "1147"
  top: "844"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_160"
  type: "Convolution"
  bottom: "844"
  top: "1150"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_161"
  type: "ReLU"
  bottom: "1150"
  top: "847"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Concat_162"
  type: "Concat"
  bottom: "765"
  bottom: "847"
  top: "848"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_163"
  type: "Convolution"
  bottom: "848"
  top: "1153"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_164"
  type: "ReLU"
  bottom: "1153"
  top: "851"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_165"
  type: "Convolution"
  bottom: "848"
  top: "1156"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_166"
  type: "ReLU"
  bottom: "1156"
  top: "854"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_167"
  type: "Convolution"
  bottom: "854"
  top: "1159"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_168"
  type: "ReLU"
  bottom: "1159"
  top: "857"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_169"
  type: "Convolution"
  bottom: "857"
  top: "1162"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 128
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_170"
  type: "ReLU"
  bottom: "1162"
  top: "860"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_171"
  type: "Convolution"
  bottom: "860"
  top: "1165"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Add_172"
  type: "Eltwise"
  bottom: "1165"
  bottom: "854"
  top: "863"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Concat_173"
  type: "Concat"
  bottom: "851"
  bottom: "863"
  top: "864"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_174"
  type: "Convolution"
  bottom: "864"
  top: "1168"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_175"
  type: "ReLU"
  bottom: "1168"
  top: "867"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_176"
  type: "Convolution"
  bottom: "815"
  top: "1171"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_177"
  type: "ReLU"
  bottom: "1171"
  top: "870"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_178"
  type: "Convolution"
  bottom: "870"
  top: "1174"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 64
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_179"
  type: "ReLU"
  bottom: "1174"
  top: "873"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_180"
  type: "Convolution"
  bottom: "873"
  top: "1177"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 64
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_181"
  type: "ReLU"
  bottom: "1177"
  top: "876"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_182"
  type: "Convolution"
  bottom: "876"
  top: "877"
  convolution_param {
    num_output: 5
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_183"
  type: "Convolution"
  bottom: "870"
  top: "1180"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 64
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_184"
  type: "ReLU"
  bottom: "1180"
  top: "880"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_185"
  type: "Convolution"
  bottom: "880"
  top: "1183"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 64
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_186"
  type: "ReLU"
  bottom: "1183"
  top: "883"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_187"
  type: "Convolution"
  bottom: "883"
  top: "884"
  convolution_param {
    num_output: 3
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Concat_188"
  type: "Concat"
  bottom: "877"
  bottom: "884"
  top: "output.1"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_189"
  type: "Convolution"
  bottom: "841"
  top: "1186"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_190"
  type: "ReLU"
  bottom: "1186"
  top: "888"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_191"
  type: "Convolution"
  bottom: "888"
  top: "1189"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 64
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_192"
  type: "ReLU"
  bottom: "1189"
  top: "891"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_193"
  type: "Convolution"
  bottom: "891"
  top: "1192"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 64
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_194"
  type: "ReLU"
  bottom: "1192"
  top: "894"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_195"
  type: "Convolution"
  bottom: "894"
  top: "895"
  convolution_param {
    num_output: 5
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_196"
  type: "Convolution"
  bottom: "888"
  top: "1195"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 64
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_197"
  type: "ReLU"
  bottom: "1195"
  top: "898"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_198"
  type: "Convolution"
  bottom: "898"
  top: "1198"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 64
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_199"
  type: "ReLU"
  bottom: "1198"
  top: "901"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_200"
  type: "Convolution"
  bottom: "901"
  top: "902"
  convolution_param {
    num_output: 3
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Concat_201"
  type: "Concat"
  bottom: "895"
  bottom: "902"
  top: "output.2"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_202"
  type: "Convolution"
  bottom: "867"
  top: "1201"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_203"
  type: "ReLU"
  bottom: "1201"
  top: "906"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_204"
  type: "Convolution"
  bottom: "906"
  top: "1204"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 64
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_205"
  type: "ReLU"
  bottom: "1204"
  top: "909"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_206"
  type: "Convolution"
  bottom: "909"
  top: "1207"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 64
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_207"
  type: "ReLU"
  bottom: "1207"
  top: "912"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_208"
  type: "Convolution"
  bottom: "912"
  top: "913"
  convolution_param {
    num_output: 5
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_209"
  type: "Convolution"
  bottom: "906"
  top: "1210"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 64
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_210"
  type: "ReLU"
  bottom: "1210"
  top: "916"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_211"
  type: "Convolution"
  bottom: "916"
  top: "1213"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 64
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_212"
  type: "ReLU"
  bottom: "1213"
  top: "919"
  relu_param {
    negative_slope: 0.009999999776482582
  }
}
layer {
  name: "Conv_213"
  type: "Convolution"
  bottom: "919"
  top: "920"
  convolution_param {
    num_output: 3
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Concat_214"
  type: "Concat"
  bottom: "913"
  bottom: "920"
  top: "output.3"
  concat_param {
    axis: 1
  }
}
